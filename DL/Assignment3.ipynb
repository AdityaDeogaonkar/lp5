{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6dfWGBhqHgX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
        "from sklearn.model_selection import GridSearchCV, ShuffleSplit, train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDKZ5a2irs53",
        "outputId": "7f536fa2-f274-48f0-ee27-7d96ccb19920"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxKr9IvctEJ9"
      },
      "outputs": [],
      "source": [
        "label_names = ['T-shirt/top','Trousers','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "# Now, let us normalize the array values\n",
        "\n",
        "train_images = train_images/np.float32(255)\n",
        "test_images = test_images/np.float32(255)\n",
        "\n",
        "# For classification purposes in python, it will be much better to convert our 2d array into a 1d array, filling in row by row.\n",
        "# Then we turn our flattened array into a numpy array object. \n",
        "def convert(nparray):\n",
        "    l=[]\n",
        "    dims=nparray.shape\n",
        "    for i in range(dims[0]):\n",
        "        l.append(nparray[i].flatten())\n",
        "    l=np.array(l)\n",
        "    return l\n",
        "train_images_mod = convert(train_images)\n",
        "test_images_mod  = convert(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqaRBsxCrbwh",
        "outputId": "9c2bf39f-0054-415b-cf1a-355694b788e5"
      },
      "outputs": [],
      "source": [
        "cnn_train       = train_images_mod.reshape(60000,28,28,1)\n",
        "cnn_test        = test_images_mod.reshape(10000,28,28,1)\n",
        "cnn_label_train = to_categorical(train_labels)\n",
        "cnn_label_test  = to_categorical(test_labels)\n",
        "\n",
        "# We will also introduce a validation set. In our case, it will be equal to 16% percent of the training set (closest to 10.000)\n",
        "# points from the test set\n",
        "\n",
        "cnn_train, cnn_train_val, cnn_label_train, cnn_label_train_val = train_test_split(cnn_train,cnn_label_train, test_size = 0.16, random_state = 2019)\n",
        "\n",
        "# The model we will build is sequential, going layer by layer. It is also the easiest way to build a model in KERAS.\n",
        "\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu' , kernel_initializer = 'he_uniform', input_shape = (28,28,1)))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128,activation = 'relu'))\n",
        "cnn_model.add(Dense(10,activation='softmax'))\n",
        "cnn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Here, we are going to fit the model to our dataset.\n",
        "\n",
        "cnn_model_training = cnn_model.fit(cnn_train,cnn_label_train,batch_size=64,epochs=50,verbose=1,\n",
        "                           validation_data=(cnn_train_val,cnn_label_train_val))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
